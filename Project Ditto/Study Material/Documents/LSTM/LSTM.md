---
태그:
  - Economic computers
생성자: Pinata
생성 일시: Invalid date
Resource: https://hanyang.dcollection.net/public_resource/pdf/200000486149_20250212172211.pdf
---
**LSTM : Vanishing Gradient Problem 해결한 순환신경망 모델**

Cell state 추가 → 상호작용하는 4개의 레이어

이 구조의 반복

  

**“**$\odot$**”는 Hadamard product (아다마르 곱)**

![[스크린샷_2025-02-12_172842.png]]

  

**LSTM 계산 과정**

- **망각 게이트 (Forget Gate)**
    - $f_t = \sigma (W_{xh\_f}x_t + W_{hh\_f} h_{t-1} + b_{h\_f})$
    - 과거 정보를 얼마나 잊을지를 결정하는 역할.
    - 입력 xt (현재 입력)과 이전 상태 ht−1을 기반으로 시그모이드(σ)를 적용해 0~1 사이의 값으로 조절.
- **입력 게이트 (Input Gate)**
    - $i_t = \sigma(W_{xh\_i} x_t + W_{hh\_i}h_{t-1} + b_{h\_i})$
    - 현재 입력을 얼마나 반영할지를 결정.
    - 시그모이드 함수를 통해 0~1 사이의 값을 가짐.
- **출력 게이트 (Output Gate)**
    - $o_t = \sigma(W_{xh\_o} x_t + W_{hh\_o} h_{t-1} + b_{h_o})$
    - 최종적으로 어떤 정보를 출력할지를 결정.
- **새로운 기억 후보값 (Cell Candidate)**
    - $g_t = \tanh(W_{xh\_g} x_t + W_{hh\_g}h_{t-1} + b_{h\_g})$
    - 현재 입력과 이전 상태를 이용하여 새로운 후보 기억값을 생성.
    - 하이퍼볼릭 탄젠트(tanh) 함수를 사용하여 -1 ~ 1 사이의 값을 가짐.
- **셀 상태 업데이트 (Cell State Update)**
    - $c_t = f_t \odot c_{t-1} + i_t \odot g_t$
    - 이전 셀 상태 $c_{t−1}$에서 잊어야 할 부분을 망각 게이트 ft로 조절.
    - 새로운 기억을 입력 게이트 it와 후보값 gt를 통해 추가.
- **은닉 상태 업데이트 (Hidden State Update)**
    - $h_t = o_t \odot \tanh (c_t)$
    - 최종적으로 ct에 하이퍼볼릭 탄젠트를 적용한 후, 출력 게이트 ot를 곱해서 은닉 상태를 결정.

  

## **1.** $x_t$ **(입력 벡터)**

- LSTM에 들어오는 **현재 시점의 입력 데이터**.
- 예제:
    - **자연어 처리(NLP)**: 단어 임베딩 벡터 (예: "hello" → 300차원 벡터)
    - **시계열 데이터**: 주식 가격, 온도 데이터 등 (예: 하루 동안의 기온 값)
    - **이미지 처리**: CNN의 특징 맵 (예: CNN을 거친 이미지 특성 벡터)
- 크기: $(n_x×1)$
    - $n_x$ = 입력 차원 수

---

## **2.** $h_{t-1}$ **(이전 은닉 상태)**

- 이전 타임스텝에서 출력된 **은닉 상태 벡터**.
- LSTM은 이전 정보(메모리)를 유지하기 위해 이 값을 사용함.
- 예제:
    - **자연어 처리**: 문장의 흐름을 기억하는 벡터
    - **시계열 예측**: 지난 날의 주식 변동 패턴을 유지하는 벡터
- 크기: $(n_h \times 1)$
    - $n_h$ = 은닉 상태 차원 수 (LSTM 유닛 개수)

---

## **3.** $W_{xh}$ **(입력에 대한 가중치 행렬)**

- 입력 $x_t$ 를 게이트 계산에 맞게 변환하는 **가중치 행렬**.
- LSTM은 4가지 게이트를 사용하므로 각각의 가중치 행렬이 있음:
    - $W_{xh_f}$ (망각 게이트 가중치)
    - $W_{xh_i}$ (입력 게이트 가중치)
    - $W_{xh_o}$ (출력 게이트 가중치)
    - $W_{xh_g}$ (셀 후보값 가중치)
- 크기: $$ n_h /times n_x $$
    - nhn_hnh = 은닉 상태 차원 수
    - nxn_xnx​ = 입력 차원 수
- 역할: 입력 벡터 xt​ 를 각 게이트의 연산에 적합한 형태로 변환.
    
    xtx_t
    

---

## **4.** $W_{hh}$ **(이전 은닉 상태에 대한 가중치 행렬)**

- 이전 은닉 상태 $h_{t-1}$ 를 게이트 계산에 맞게 변환하는 **가중치 행렬**.
- 4가지 게이트별로 가중치가 있음:
    - $W_{hh_f}$ (망각 게이트 가중치)
    - $W_{hh_i}$ (입력 게이트 가중치)
    - $W_{hh_o}$ (출력 게이트 가중치)
    - $W_{hh_g}$ (셀 후보값 가중치)
- 크기: $n_h×n_h$
    - $n_h$ = 은닉 상태 차원 수
- 역할: 이전 은닉 상태 $h_{t-1}$ 를 게이트의 연산에 맞게 변환.

---

## **5.** $b_h$ **(편향 값)**

- 각 게이트 연산에서 더해지는 **편향(bias) 값**.
- 4개의 게이트마다 편향 값이 있음:
    - $b_{h_f}$ (망각 게이트 편향)
    - $b_{h_i}$ (입력 게이트 편향)
    - $b_{h_o}$ (출력 게이트 편향)
    - $b_{h_g}$ (셀 후보값 편향)
- 크기: $n_h \times 1$
    - $n_h$ = 은닉 상태 차원 수
- 역할: 모델이 더 유연하게 학습할 수 있도록 도움.

---

## **6.** $f_t, i_t, o_t$ **(게이트 값들)**

- **게이트 값**은 시그모이드(σ)를 적용하여 0~1 사이의 값을 가짐.
    
    σ\sigma
    
    - $f_t$: 과거 정보를 **얼마나 잊을지** 결정하는 망각 게이트.
    - $i_t$: 현재 입력을 **얼마나 반영할지** 결정하는 입력 게이트.
    - $o_t$: 현재 셀 상태에서 **출력할 정보를 선택**하는 출력 게이트.
- 크기: $n_h \times 1$

---

## **7.** $g_t$ **(셀 후보값)**

- 새로운 정보를 생성하는 **셀 후보값**.
- 하이퍼볼릭 탄젠트($tanh$)를 적용하여 -1 ~ 1 범위를 가짐.
- 크기: $nh×1$

---

## **8.** $c_t$ **(셀 상태)**

- **장기 기억을 저장하는 역할**을 하는 벡터.
- 크기: $n_h×1$
- 업데이트 식:
    
    $c_t = f_t \odot c_{t-1} + i_t \odot g_t$
    
    - 과거 정보를 유지할지 ($f_t \odot c_{t-1}$)
    - 새로운 정보를 추가할지 ($i_t \odot g_t$)

---

## **9.** $h_t$ **(은닉 상태)**

- 현재 시점에서 LSTM이 출력하는 **은닉 상태 벡터**.
- 크기: $n_h×1$
- 업데이트 식:  
    $h_t = o_t \odot \tanh(c_t)$
    - 현재 셀 상태 $c_t$ 를 기반으로 최종 출력값 결정.