{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: ./data/macro\\fedfunds.csv\n",
      "            interest_rate\n",
      "date                     \n",
      "1954-07-01           0.80\n",
      "1954-08-01           1.22\n",
      "1954-09-01           1.07\n",
      "1954-10-01           0.85\n",
      "1954-11-01           0.83\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "\n",
    "# 1. API í‚¤ ì„¤ì • (ë³€ìˆ˜ë¡œ!)\n",
    "my_api_key = '40e75c608c24193ed3c91fa450715041'\n",
    "fred = Fred(api_key=my_api_key)\n",
    "\n",
    "# 2. ê°€ì ¸ì˜¬ ì‹œë¦¬ì¦ˆ ID ì„¤ì • (ì—¬ê¸°ì„  ì—°ë°©ê¸ˆë¦¬)\n",
    "series_id = 'FEDFUNDS'\n",
    "interest_rate = fred.get_series(series_id)\n",
    "\n",
    "# 3. pandas DataFrameìœ¼ë¡œ ë³€í™˜ (ë‚ ì§œ í¬í•¨)\n",
    "df = pd.DataFrame(interest_rate)\n",
    "df.columns = ['interest_rate']\n",
    "df.index.name = 'date'\n",
    "\n",
    "# 4. ì €ì¥ ë””ë ‰í† ë¦¬ ë° íŒŒì¼ëª… ì„¤ì •\n",
    "save_dir = '../data/macro'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f'{series_id.lower()}.csv')\n",
    "\n",
    "# 5. CSVë¡œ ì €ì¥\n",
    "df.to_csv(save_path)\n",
    "print(f'Data saved to: {save_path}')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Obtaining income data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.67it/s]\n",
      "Obtaining balance data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.77it/s]\n",
      "Obtaining cashflow data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL ì¬ë¬´ì œí‘œ ì €ì¥ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "from financetoolkit import Toolkit\n",
    "import os\n",
    "\n",
    "# ğŸ“Œ ì—¬ê¸°ì— ë„ˆì˜ FMP API í‚¤ ì…ë ¥\n",
    "api_key = \"fAtjTuyk4qp6hk3rxKMbshOQKSlc6rZh\"  # ì˜ˆ: \"c1f5e2aefed14714ac12c0dd08e774a9\"\n",
    "\n",
    "# ğŸ” ìˆ˜ì§‘í•  ê¸°ì—… í‹°ì»¤ (ì—¬ëŸ¬ ê°œë„ ê°€ëŠ¥)\n",
    "companies = \"AAPL\"\n",
    "\n",
    "toolkit = Toolkit(\n",
    "    companies,\n",
    "    api_key=api_key,\n",
    "    start_date=\"2018-01-01\",  # ì›í•˜ëŠ” ì‹œì‘ ë‚ ì§œ\n",
    "    quarterly=False           # True: ë¶„ê¸° / False: ì—°ê°„\n",
    ")\n",
    "\n",
    "# ğŸ“ ì €ì¥ í´ë” ìƒì„±\n",
    "save_dir = f\"../data/firm/{companies}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# ğŸ“Š ì¬ë¬´ì œí‘œ ìˆ˜ì§‘\n",
    "income_statement = toolkit.get_income_statement()\n",
    "balance_sheet = toolkit.get_balance_sheet_statement()\n",
    "cashflow = toolkit.get_cash_flow_statement()\n",
    "\n",
    "# ğŸ’¾ CSVë¡œ ì €ì¥\n",
    "balance_sheet.to_csv(f\"{save_dir}/balance_sheet.csv\")\n",
    "income_statement.to_csv(f\"{save_dir}/income_statement.csv\")\n",
    "cashflow.to_csv(f\"{save_dir}/cashflow.csv\")\n",
    "\n",
    "print(f\"{companies} ì¬ë¬´ì œí‘œ ì €ì¥ ì™„ë£Œ.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë‰´ìŠ¤ 10ê±´ ì €ì¥ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "api_key = \"2b42d24318ea75778896dfaafd4f9b35\"\n",
    "query = \"Apple Inc\"\n",
    "url = f\"https://gnews.io/api/v4/search?q={query}&lang=en&max=100&token={api_key}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "if \"articles\" in data:\n",
    "    articles = data[\"articles\"]\n",
    "    df = pd.DataFrame(articles)\n",
    "    df[\"publishedAt\"] = pd.to_datetime(df[\"publishedAt\"]).dt.date  # ë‚ ì§œë§Œ ì¶”ì¶œ\n",
    "    save_dir = \"../data/firm/AAPL\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    df.to_csv(f\"{save_dir}/news_raw.csv\", index=False)\n",
    "    print(f\"âœ… ë‰´ìŠ¤ {len(df)}ê±´ ì €ì¥ ì™„ë£Œ.\")\n",
    "else:\n",
    "    print(\"âŒ ê¸°ì‚¬ ì—†ìŒ or API ì¸ì¦ ë¬¸ì œ:\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FinBERT ê°ì • ë¶„ì„ ë° ì €ì¥ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# ë‰´ìŠ¤ ë¡œë“œ (news_raw.csvì—ì„œ)\n",
    "df = pd.read_csv(\"../data/firm/AAPL/news_raw.csv\")\n",
    "df[\"publishedAt\"] = pd.to_datetime(df[\"publishedAt\"]).dt.date\n",
    "\n",
    "# ê°ì • ë¶„ì„ ì‹¤í–‰ (ì œëª© ê¸°ì¤€)\n",
    "df[\"finbert_sentiment\"] = df[\"title\"].apply(lambda x: nlp(x)[0][\"label\"])\n",
    "\n",
    "# ë‚ ì§œë³„ ê°ì • ë¶„í¬ ìš”ì•½\n",
    "daily_sentiment = df.groupby([\"publishedAt\", \"finbert_sentiment\"]).size().unstack(fill_value=0)\n",
    "daily_sentiment = daily_sentiment.reset_index()\n",
    "daily_sentiment.columns.name = None\n",
    "daily_sentiment.rename(columns={\"publishedAt\": \"date\"}, inplace=True)\n",
    "\n",
    "# ì €ì¥\n",
    "save_path = \"../data/firm/AAPL/news_sentiment_finbert.csv\"\n",
    "daily_sentiment.to_csv(save_path, index=False)\n",
    "print(\"âœ… FinBERT ê°ì • ë¶„ì„ ë° ì €ì¥ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  interest_rate  negative  neutral\n",
      "0 1954-07-01            0.8       0.0      0.0\n",
      "1 1954-08-01           1.22       0.0      0.0\n",
      "2 1954-09-01           1.07       0.0      0.0\n",
      "3 1954-10-01           0.85       0.0      0.0\n",
      "4 1954-11-01           0.83       0.0      0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ê¸ˆë¦¬ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "macro = pd.read_csv(\"../data/macro/fedfunds.csv\")\n",
    "macro[\"date\"] = pd.to_datetime(macro[\"date\"])\n",
    "\n",
    "# ê¸°ì—… ë‰´ìŠ¤ ê°ì • ë°ì´í„°\n",
    "firm_sentiment = pd.read_csv(\"../data/firm/AAPL/news_sentiment_finbert.csv\")\n",
    "firm_sentiment[\"date\"] = pd.to_datetime(firm_sentiment[\"date\"])\n",
    "\n",
    "# ë‚ ì§œ ê¸°ì¤€ ë³‘í•© (inner ë˜ëŠ” outer join ê°€ëŠ¥)\n",
    "df = pd.merge(macro, firm_sentiment, on=\"date\", how=\"outer\")\n",
    "\n",
    "# ê²°ì¸¡ê°’ ì²˜ë¦¬ (ì˜ˆ: 0ìœ¼ë¡œ ëŒ€ì²´)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# í™•ì¸\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: RS_2023-01.zst ...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 521: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(save_path):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[43mwget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# ğŸ“Œ Step 3: í•„ìš”í•œ ë°ì´í„°ë§Œ í•„í„°ë§\u001b[39;00m\n\u001b[32m     22\u001b[39m TARGET_SUBREDDIT = \u001b[33m\"\u001b[39m\u001b[33mwallstreetbets\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cyong\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\wget.py:526\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(url, out, bar)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    525\u001b[39m     binurl = url\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m (tmpfile, headers) = \u001b[43mulib\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinurl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmpfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m filename = detect_filename(url, out, headers)\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outdir:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cyong\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py:240\u001b[39m, in \u001b[36murlretrieve\u001b[39m\u001b[34m(url, filename, reporthook, data)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[32m    225\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m \u001b[33;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m url_type, path = _splittype(url)\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m contextlib.closing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    241\u001b[39m     headers = fp.info()\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[32m    244\u001b[39m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cyong\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cyong\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py:521\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    520\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cyong\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py:630\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    628\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cyong\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py:559\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    558\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cyong\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    491\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cyong\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py:639\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 521: "
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# ğŸ“Œ Step 1: Reddit ì•„ì¹´ì´ë¸Œ ë‹¤ìš´ë¡œë“œ ì„¤ì •\n",
    "month = \"2023-01\"  # ì›í•˜ëŠ” ì›” ì„ íƒ\n",
    "file_name = f\"RS_{month}.zst\"\n",
    "url = f\"https://files.pushshift.io/reddit/submissions/{file_name}\"\n",
    "save_path = f\"../data/individual/{file_name}\"\n",
    "output_path = f\"../data/individual/wsb_sentiment_{month}.csv\"\n",
    "\n",
    "# ğŸ“¥ Step 2: íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "if not os.path.exists(save_path):\n",
    "    print(f\"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {file_name} ...\")\n",
    "    wget.download(url, out=save_path)\n",
    "\n",
    "# ğŸ“Œ Step 3: í•„ìš”í•œ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "TARGET_SUBREDDIT = \"wallstreetbets\"\n",
    "KEYWORDS = [\"TSLA\", \"Tesla\", \"AAPL\", \"Apple\"]\n",
    "DATE_RANGE = (\"2023-01-10\", \"2023-01-20\")  # ì›í•˜ëŠ” ë‚ ì§œ ë²”ìœ„\n",
    "\n",
    "records = []\n",
    "\n",
    "with open(save_path, 'rb') as f:\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    stream_reader = dctx.stream_reader(f)\n",
    "\n",
    "    for line in tqdm(stream_reader):\n",
    "        try:\n",
    "            post = json.loads(line)\n",
    "            # ì„œë¸Œë ˆë”§ í•„í„°\n",
    "            if post.get(\"subreddit\") != TARGET_SUBREDDIT:\n",
    "                continue\n",
    "\n",
    "            # í‚¤ì›Œë“œ í•„í„° (ì œëª© + ë³¸ë¬¸)\n",
    "            title = post.get(\"title\", \"\")\n",
    "            selftext = post.get(\"selftext\", \"\")\n",
    "            if not any(keyword.lower() in (title + selftext).lower() for keyword in KEYWORDS):\n",
    "                continue\n",
    "\n",
    "            # ë‚ ì§œ í•„í„°\n",
    "            timestamp = pd.to_datetime(post.get(\"created_utc\", 0), unit='s')\n",
    "            if not (DATE_RANGE[0] <= str(timestamp.date()) <= DATE_RANGE[1]):\n",
    "                continue\n",
    "\n",
    "            # í•„ìš”í•œ ë°ì´í„°ë§Œ ì €ì¥\n",
    "            records.append({\n",
    "                \"date\": timestamp.date(),\n",
    "                \"title\": title,\n",
    "                \"text\": selftext\n",
    "            })\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# ğŸ“Œ Step 4: DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# ğŸ§  Step 5: ê°ì • ë¶„ì„ (VADER)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df[\"sentiment\"] = df[\"title\"].apply(lambda x: analyzer.polarity_scores(str(x))[\"compound\"])\n",
    "\n",
    "# ğŸ“Š Step 6: ë‚ ì§œë³„ í‰ê·  ê°ì • ì ìˆ˜ ê³„ì‚°\n",
    "daily_sentiment = df.groupby(\"date\")[\"sentiment\"].mean().reset_index()\n",
    "daily_sentiment.columns = [\"date\", \"avg_reddit_sentiment\"]\n",
    "\n",
    "# ğŸ’¾ Step 7: ìµœì¢… ì €ì¥\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "daily_sentiment.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… í•„í„°ë§ëœ ê²Œì‹œê¸€ {len(df)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ â†’ ê°ì • ë¶„ì„ ì™„ë£Œ â†’ {output_path} ì €ì¥!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ê²½ë¡œëŠ” í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •\n",
    "macro = pd.read_csv(\"../data/macro/fedfunds.csv\")  # êµ­ê°€ ì—ì´ì „íŠ¸\n",
    "macro[\"date\"] = pd.to_datetime(macro[\"date\"])\n",
    "\n",
    "news = pd.read_csv(\"../data/firm/AAPL/news_sentiment_finbert.csv\")  # ê¸°ì—…\n",
    "news[\"date\"] = pd.to_datetime(news[\"date\"])\n",
    "\n",
    "reddit = pd.read_csv(\"../data/individual/wsb_sentiment_2023-01.csv\")  # ê°œì¸\n",
    "reddit[\"date\"] = pd.to_datetime(reddit[\"date\"])\n",
    "\n",
    "# íƒ€ê²Ÿ ìƒì„±ìš© ì£¼ê°€\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "price_df = yf.download(\"AAPL\", start=\"2022-12-31\", end=\"2023-01-25\", progress=False)\n",
    "price_df = price_df[[\"Close\"]].reset_index()\n",
    "price_df.columns = [\"date\", \"close\"]\n",
    "price_df[\"date\"] = pd.to_datetime(price_df[\"date\"])\n",
    "price_df[\"log_return\"] = np.log(price_df[\"close\"].shift(-1) / price_df[\"close\"])\n",
    "price_df[\"target_binary\"] = (price_df[\"log_return\"] > 0).astype(int)\n",
    "target = price_df[[\"date\", \"log_return\", \"target_binary\"]]\n",
    "\n",
    "# ë³‘í•©\n",
    "merged = pd.merge(macro, news, on=\"date\", how=\"outer\")\n",
    "merged = pd.merge(merged, reddit, on=\"date\", how=\"outer\")\n",
    "merged = pd.merge(merged, target, on=\"date\", how=\"inner\")\n",
    "\n",
    "# ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "merged.fillna(0, inplace=True)\n",
    "merged = merged.sort_values(\"date\")\n",
    "\n",
    "# ì €ì¥\n",
    "os.makedirs(\"./data/merged\", exist_ok=True)\n",
    "merged.to_csv(\"./data/merged/final_model_input.csv\", index=False)\n",
    "\n",
    "print(\"âœ… final_model_input.csv ìƒì„± ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"./data/merged/final_model_input.csv\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "# 2. ì…ë ¥ & íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "X = df.drop(columns=[\"date\", \"log_return\", \"target_binary\"])\n",
    "y = df[\"target_binary\"].values\n",
    "\n",
    "# 3. ì—ì´ì „íŠ¸ë³„ ì»¬ëŸ¼ ì •ì˜ (ìˆ˜ë™ ì„¤ì •)\n",
    "gov_cols = [\"interest_rate\"]\n",
    "firm_cols = [\"positive\", \"negative\", \"neutral\"]\n",
    "indiv_cols = [\"avg_reddit_sentiment\"]\n",
    "\n",
    "# 4. ê°ê° ë¶„ë¦¬\n",
    "X_gov = X[gov_cols].values\n",
    "X_firm = X[firm_cols].values\n",
    "X_indiv = X[indiv_cols].values\n",
    "\n",
    "# 5. ì •ê·œí™”\n",
    "scaler_gov = StandardScaler()\n",
    "scaler_firm = StandardScaler()\n",
    "scaler_indiv = StandardScaler()\n",
    "\n",
    "X_gov = scaler_gov.fit_transform(X_gov)\n",
    "X_firm = scaler_firm.fit_transform(X_firm)\n",
    "X_indiv = scaler_indiv.fit_transform(X_indiv)\n",
    "\n",
    "# 6. train/test split\n",
    "X_gov_train, X_gov_test, y_train, y_test = train_test_split(X_gov, y, test_size=0.2, shuffle=False)\n",
    "X_firm_train, X_firm_test = train_test_split(X_firm, test_size=0.2, shuffle=False)\n",
    "X_indiv_train, X_indiv_test = train_test_split(X_indiv, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "def to_tensor(arr):\n",
    "    return torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "# (batch, agents, features)\n",
    "X_train = torch.stack([\n",
    "    to_tensor(X_gov_train),\n",
    "    to_tensor(X_firm_train),\n",
    "    to_tensor(X_indiv_train)\n",
    "], dim=1)\n",
    "\n",
    "X_test = torch.stack([\n",
    "    to_tensor(X_gov_test),\n",
    "    to_tensor(X_firm_test),\n",
    "    to_tensor(X_indiv_test)\n",
    "], dim=1)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = data.TensorDataset(X_train, y_train)\n",
    "test_dataset = data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# ì—ì´ì „íŠ¸ íƒ€ì… ì¸ë±ìŠ¤\n",
    "agent_types = torch.tensor([0, 1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AgentInteractionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_agent_types):\n",
    "        super().__init__()\n",
    "        self.relation_matrix = nn.Parameter(torch.randn(num_agent_types, num_agent_types))\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x, agent_types):\n",
    "        batch_size, num_agents, input_dim = x.size()\n",
    "        interaction = torch.zeros_like(x)\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            for j in range(num_agents):\n",
    "                w_ij = self.relation_matrix[agent_types[i], agent_types[j]]\n",
    "                interaction[:, i, :] += w_ij * x[:, j, :]\n",
    "\n",
    "        return F.relu(self.linear(x + interaction))\n",
    "\n",
    "class AgentBasedPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, num_agent_types, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.agent_layer = AgentInteractionLayer(input_dim, num_agent_types)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim * num_agent_types, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, agent_types):\n",
    "        x = self.agent_layer(x, agent_types)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_dim = X_train.shape[-1]\n",
    "model = AgentBasedPredictor(input_dim=input_dim, num_agent_types=3, hidden_dim=64)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# í•™ìŠµ\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch, agent_types)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch, agent_types)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "acc = correct / total\n",
    "print(f\"âœ… Test Accuracy: {acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ì˜ˆì¸¡ vs ì‹¤ì œ ì €ì¥\n",
    "preds_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch, agent_types)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        preds_list.extend(preds.tolist())\n",
    "\n",
    "# ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°\n",
    "dates = df[\"date\"].iloc[-len(preds_list):].reset_index(drop=True)\n",
    "actual = y_test.numpy()\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(dates, actual, label=\"Actual\", marker='o')\n",
    "plt.plot(dates, preds_list, label=\"Predicted\", marker='x')\n",
    "plt.legend()\n",
    "plt.title(\"ğŸ“ˆ Actual vs Predicted (Up/Down)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Direction (0=down, 1=up)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
